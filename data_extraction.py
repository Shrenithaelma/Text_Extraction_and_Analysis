# -*- coding: utf-8 -*-
"""DATA_EXTRACTION.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tMCJpUAggYk2kXngPNRUgRBViyE6JVvD
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
file_path="/content/drive/MyDrive/Blackcoffer_assignment/Input.xlsx"

data_frame = pd.read_excel(file_path)

data_frame

"""FINDING INVALID URLS"""

def is_404(url):
    response = requests.get(url)
    return response.status_code == 404

# Finding URLs that give 404 page error
invalid_urls_df = data_frame[data_frame['URL'].apply(is_404)]

print("URLs with 404 page error and their url_id:")
print(invalid_urls_df[['URL_ID', 'URL']])

"""VALID URLS"""

import requests
def is_valid_url(url):
    try:
        response = requests.get(url)
        return response.status_code == 200
    except requests.exceptions.RequestException:
        return False

# Creating a new DataFrame with valid URLs
valid_urls = data_frame[data_frame['URL'].apply(is_valid_url)]
valid_urls

"""EXTRACTING TITLE,ARTICLE TEXT"""

from bs4 import BeautifulSoup
import re
from urllib.request import urlopen
def extract_article_text(url):
    try:
        response = requests.get(url)
        response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)

        soup = BeautifulSoup(response.content, 'html.parser')

        # Removing script, style, and unnecessary tags
        for element in soup(["script", "style", "nav", "footer", "header", "aside", "iframe"]):
            element.extract()

        containers = soup.find_all("div")
        most_prominent_container = max(containers, key=lambda container: len(container.find_all("p", recursive=False)))
        paragraphs = most_prominent_container.find_all("p")
        article_text = " ".join(paragraph.get_text() for paragraph in paragraphs)
        article_text = re.sub(r"\s+", " ", article_text).strip()
        return article_text

    except requests.exceptions.RequestException as e:
        print(f"Error fetching URL: {url}")
        print(f"Error message: {str(e)}")
        return None
    except Exception as e:
        print(f"Error parsing content from URL: {url}")
        print(f"Error message: {str(e)}")
        return None

def main(xlsx_file_path):
    try:
        df = pd.read_excel(xlsx_file_path, engine='openpyxl')

        # Creating new columns to store the article text and title
        df['Article Text'] = None
        df['Title'] = None

        for index, row in df.iterrows():
            url_id = row['URL_ID']
            url = row['URL']

            if is_valid_url(url):
                article_text = extract_article_text(url)
                if article_text is not None:
                    soup = BeautifulSoup(requests.get(url).content, 'html.parser')
                    title = soup.title.get_text()
                    df.at[index, 'Article Text'] = article_text
                    df.at[index, 'Title'] = title

                    print(f"URL_ID: {url_id}")
                    print(f"URL: {url}")
                    print(f"Title: {title}")
                    print(f"Article Text: {article_text}")
                    print("=" * 50)

        return df

    except Exception as e:
        print(f"Error reading the XLSX file: {e}")

xlsx_file_path ='/content/drive/MyDrive/Blackcoffer_assignment/Input.xlsx'
valid_urls_df = main(xlsx_file_path)

print(valid_urls_df)

"""REMOVING STOP WORDS"""

with open("/content/drive/MyDrive/Blackcoffer_assignment/stopwords.txt", "r") as f:
    stop_words = set(f.read().splitlines())

import pandas as pd
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

def remove_stop_words(article_text):
    words = word_tokenize(article_text)
    without_stop_words = [word.lower() for word in words if word.lower() not in stop_words]
    return " ".join(without_stop_words)

import nltk
nltk.download('punkt')

valid_urls_df["cleaned_article_text"] = valid_urls_df["Article Text"].astype(str).apply(remove_stop_words)

valid_urls_df

"""POSITIVE WORDS COUNT"""

with open("/content/drive/MyDrive/Blackcoffer_assignment/positive-words.txt", "r") as f:
    positive_words = set(f.read().splitlines())

def count_positive_words(text):
    words = word_tokenize(text.lower())
    positive_word_count = sum(1 for word in words if word in positive_words)
    return positive_word_count

valid_urls_df["positive_word_count"] = valid_urls_df["cleaned_article_text"].apply(count_positive_words)

"""NEGATIVE WORDS COUNT"""

with open("/content/drive/MyDrive/Blackcoffer_assignment/negative-words.txt", "r", encoding="ISO-8859-1") as f:
    negative_words = set(f.read().splitlines())

def count_negative_words(text):
    words = word_tokenize(text.lower())
    negative_word_count = sum(1 for word in words if word in negative_words)
    return negative_word_count

valid_urls_df["negative_word_count"] = valid_urls_df["cleaned_article_text"].apply(count_negative_words)

"""CREATING OUTPUT DATAFRAME"""

output=valid_urls_df[['URL_ID','URL']].copy()

output['POSITIVE SCORE']=None
output['NEGATIVE SCORE']=None
output["POLARITY SCORE"]=None
output["SUBJECTIVITY SCORE"]=None
output['AVG SENTENCE LENGTH']=None
output['PERCENTAGE OF COMPLEX WORDS']=None
output['FOG INDEX']=None
output['AVG NUMBER OF WORDS PER SENTENCE']=None
output['COMPLEX WORD COUNT']=None
output['WORD COUNT']=None
output['SYLLABLE PER WORD']=None
output['PERSONAL PRONOUNS']=None
output['AVG WORD LENGTH']=None

"""CALCULATING POSITIVE SCORE,NEGATIVE SCORE AND POLARITY SCORE"""

output['POSITIVE SCORE']=valid_urls_df['positive_word_count']
output['NEGATIVE SCORE']=valid_urls_df['negative_word_count']
output["POLARITY SCORE"] = (output['POSITIVE SCORE'] - output['NEGATIVE SCORE']) / ((output['POSITIVE SCORE'] + output['NEGATIVE SCORE']) + 0.000001)

"""CALCULATING NUMBER OF WORDS"""

from nltk.tokenize import word_tokenize

def count_words(text):
    words = word_tokenize(text)
    return len(words)

valid_urls_df["num_words"] = valid_urls_df["cleaned_article_text"].apply(count_words)

valid_urls_df["num_words"]

"""CALCULATING NUMBER OF SENTENCES"""

from nltk.tokenize import sent_tokenize

def count_sentences(text):
    sentences = sent_tokenize(text)
    return len(sentences)

valid_urls_df["num_sentences"] = valid_urls_df["cleaned_article_text"].apply(count_sentences)

"""CALCULATING SUBJECTIVITY SCORE AND AVG SENTENCE LENGTH"""

output["SUBJECTIVITY SCORE"]=(output['POSITIVE SCORE'] + output['NEGATIVE SCORE'])/ ((valid_urls_df['num_words']) + 0.000001)

output['AVG SENTENCE LENGTH']=valid_urls_df["num_words"]/valid_urls_df["num_sentences"]

"""CALCULATING COMPLEX WORDS COUNT"""

import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import cmudict
nltk.download('punkt')
nltk.download('cmudict')
nltk.download('punkt')
nltk.download('cmudict')

def count_complex_words(article_text, complexity_threshold=7):
    if isinstance(article_text, str):
        words = re.findall(r'\b\w+\b', article_text)
        complex_words_count = sum(1 for word in words if len(word) > complexity_threshold)
        return complex_words_count
    else:
        return 0
valid_urls_df['Complex Words Count'] = valid_urls_df['cleaned_article_text'].apply(count_complex_words)

"""CALCULATING PERCENTAGE OF COMPLEX WORDS,FOG INDEX,COMPLEX WORD COUNT AND WORD COUNT"""

output['PERCENTAGE OF COMPLEX WORDS']=valid_urls_df['Complex Words Count']/valid_urls_df['num_words']
output['FOG INDEX']= 0.4 * (output['AVG SENTENCE LENGTH']+output['PERCENTAGE OF COMPLEX WORDS'])
output['COMPLEX WORD COUNT']=valid_urls_df['Complex Words Count']
output['WORD COUNT']=valid_urls_df['num_words']

"""CALCULATING AVG WORDS PER SENTENCE"""

from nltk.tokenize import sent_tokenize, word_tokenize

def average_words_per_sentence(text):
    sentences = sent_tokenize(text)
    word_counts = [len(word_tokenize(sentence)) for sentence in sentences]
    return sum(word_counts) / len(word_counts) if len(word_counts) > 0 else 0
valid_urls_df["avg_words_per_sentence"] = valid_urls_df["cleaned_article_text"].apply(average_words_per_sentence)

"""CALCULATING SYLLABLES PER WORD"""

from nltk.tokenize import word_tokenize

def count_syllables(word):
    exceptions = ["es", "ed"]
    if word[-2:] in exceptions:
        return max(1, len(word_tokenize(word)) - 1)
    else:
        return len(word_tokenize(word))

valid_urls_df["syllables_per_word"] = valid_urls_df["cleaned_article_text"].apply(lambda text: sum(count_syllables(word) for word in word_tokenize(text)) / max(len(word_tokenize(text)), 1))

"""CALCULATING PERSONAL PRONOUS COUNT"""

def count_personal_pronouns(text):
  pronoun_count = re.compile(r'\b(I|we|ours|my|mine|(?-i:us))\b', re.I)
  pronouns = pronoun_count.findall(text)
  return len(pronouns)

valid_urls_df["personal_pronouns_count"] = valid_urls_df["Article Text"].astype(str).apply(count_personal_pronouns)

"""CALCULATING AVG WORD LENGTH"""

from nltk.tokenize import word_tokenize

def average_word_length(text):
    words = word_tokenize(text)
    total_characters = sum(len(word) for word in words)
    total_words = len(words)
    return total_characters / total_words if total_words > 0 else 0
valid_urls_df["avg_word_length"] = valid_urls_df["cleaned_article_text"].apply(average_word_length)

output['SYLLABLE PER WORD']=valid_urls_df["syllables_per_word"]
output['PERSONAL PRONOUNS']=valid_urls_df["personal_pronouns_count"]
output['AVG WORD LENGTH']=valid_urls_df["avg_word_length"]
output['AVG NUMBER OF WORDS PER SENTENCE']=valid_urls_df["avg_words_per_sentence"]

"""SAVING TITLE AND ARTICLE TEXT TO FILE WITH URL_ID AS FILE NAME"""

def save_article_text_to_file(url_id, title,article_text):
    if article_text is not None:
        file_name = os.path.join('/content/drive/MyDrive/Blackcoffer_assignment/Article text', f"{url_id}.txt")
        with open(file_name, "w", encoding="utf-8") as file:
            file.write(article_text)
            file.write(f"Title: {title}\n\n")
num_files_created = 0
for _, row in valid_urls_df.iterrows():
    url_id = row['URL_ID']
    title = row['Title']
    article_text = valid_urls_df[valid_urls_df['URL_ID'] == url_id]['Article Text'].values[0]
    save_article_text_to_file(url_id, title,article_text)
    num_files_created += 1

output

"""INVALID URL ROWS ARE REMOVED"""

output

"""SAVING THE OUTPUT DATAFRAME TO OUTPUT DATA SHEET"""

output.to_excel('/content/drive/MyDrive/Blackcoffer_assignment/Output Data.xlsx')

